{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Befezdow/ai_test/blob/master/course_work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvJCHydW972n",
        "colab_type": "code",
        "outputId": "d2663178-f4d4-4867-a7e3-df0df8256707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "names_array = ['gender', 'age', 'class1', 'class2', 'class3', *['t{}'.format(i + 1) for i in range(60000)]]\n",
        "url = '/content/gdrive/My Drive/ML/half.csv'\n",
        "data = pd.read_csv(url, names=names_array)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osz4aRYNQ8OJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.fillna(data.median(axis = 0), axis = 0)\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoCrlHyGBVYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "current_count_per_sensor = 5000\n",
        "needed_count_per_sensor = 100\n",
        "sensors_count = 12\n",
        "\n",
        "non_sliceable_columns = data.iloc[:,0:5]\n",
        "sliceable_columns = data.iloc[:,5:]\n",
        "\n",
        "current_offset = 0\n",
        "new_data = pd.DataFrame()\n",
        "for i in range(int(np.floor(current_count_per_sensor / needed_count_per_sensor))):\n",
        "  new_offset = current_offset + needed_count_per_sensor\n",
        "  new_slice = pd.DataFrame()\n",
        "  \n",
        "  for j in range(sensors_count):\n",
        "    index_from = j * current_count_per_sensor + current_offset\n",
        "    index_to = j * current_count_per_sensor + new_offset\n",
        "    data_slice = sliceable_columns.iloc[:, index_from:index_to]\n",
        "    new_slice = pd.concat([new_slice, data_slice], axis=1, sort=False, ignore_index=True)\n",
        "  \n",
        "  completed_rows = pd.concat([non_sliceable_columns.copy(), new_slice], axis=1, sort=False, ignore_index=True)\n",
        "  new_data = pd.concat([new_data, completed_rows], axis=0, sort=False, ignore_index=True)\n",
        "  current_offset += needed_count_per_sensor\n",
        "\n",
        "new_data.columns = ['gender', 'age', 'class1', 'class2', 'class3', *['t{}'.format(i + 1) for i in range(needed_count_per_sensor * sensors_count)]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGuUxt6mP4fJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = new_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_JmGW5BQpyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# убираем лишние классовые признаки\n",
        "data = data.drop('class2', axis = 1)\n",
        "data = data.drop('class3', axis = 1)\n",
        "\n",
        "#собираем числовые колонки\n",
        "numerical_columns = [c for c in data.columns if data[c].dtype.name != 'object' and c != 'class1']\n",
        "\n",
        "# векторизируем колонку gender\n",
        "data.at[data['gender'] == 'Male', 'gender'] = 0\n",
        "data.at[data['gender'] == 'Female', 'gender'] = 1\n",
        "\n",
        "# нормализуем числовые атрибуты\n",
        "data_numerical = data[numerical_columns]\n",
        "data_numerical = (data_numerical - data_numerical.mean(axis = 0))/data_numerical.std(axis = 0)\n",
        "data[numerical_columns] = data_numerical\n",
        "\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tJMN0mnb3wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# бьем данные на входы и выходы\n",
        "x = data.drop('class1', axis = 1)\n",
        "y = data['class1']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6KEkBMQcL27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
        "\n",
        "n_train, _ = x_train.shape \n",
        "n_test,  _ = x_test.shape \n",
        "\n",
        "print('Train dataset size: {}'.format(n_train))\n",
        "print('Test dataset size: {}'.format(n_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11EqAbCwjK-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.to_numpy()\n",
        "x_test = x_test.to_numpy()\n",
        "y_train = y_train.to_numpy()\n",
        "y_test = y_test.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBt8DnCBf1Dm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "x_train = torch.Tensor(x_train)\n",
        "y_train = torch.Tensor(y_train).long() - 1\n",
        "\n",
        "x_test = torch.Tensor(x_test)\n",
        "y_test = torch.Tensor(y_test).long() - 1\n",
        "\n",
        "print('Train X shape: {}'.format(x_train.shape))\n",
        "print('Train Y shape: {}'.format(y_train.shape))\n",
        "\n",
        "print('Test X shape: {}'.format(x_test.shape))\n",
        "print('Test Y shape: {}'.format(y_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IlSjzIzMur1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.fc1 = nn.Linear(1202, 500)\n",
        "    self.fc2 = nn.Linear(500, 500)\n",
        "    self.fc3 = nn.Linear(500, 9)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return F.log_softmax(x, dim=1)\n",
        "\n",
        "net = Net().double()\n",
        "\n",
        "# Осуществляем оптимизацию путем стохастического градиентного спуска\n",
        "# optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "# Создаем функцию потерь\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "epochs_count = 10\n",
        "batch_size = 100\n",
        "\n",
        "dataset_size = x_train.shape[0]\n",
        "batches_count = int(np.ceil(dataset_size / batch_size))\n",
        "\n",
        "for epoch_index in range(epochs_count):\n",
        "  for batch_index in range(batches_count):\n",
        "    x_var = Variable(x_train[batch_index * batch_size:batch_index * batch_size + batch_size])\n",
        "    y_var = Variable(y_train[batch_index * batch_size:batch_index * batch_size + batch_size])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    net_out = net(x_var.double())\n",
        "    loss = criterion(net_out, y_var)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # print('Train Epoch: {}; Batch: {}/{}; tLoss: {:.6f}'.format(epoch_index + 1, batch_index + 1, batches_count, loss.data.item()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWnI_wh2NbGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_size = x_test.shape[0]\n",
        "\n",
        "batch_size = 100\n",
        "batches_count = int(np.ceil(dataset_size / batch_size))\n",
        "\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "for batch_index in range(batches_count):\n",
        "    x_var = Variable(x_test[batch_index * batch_size:batch_index * batch_size + batch_size])\n",
        "    y_var = Variable(y_test[batch_index * batch_size:batch_index * batch_size + batch_size])\n",
        "\n",
        "    net_out = net(x_var.double())\n",
        "    test_loss += criterion(net_out, y_var).item()  # суммируем потери со всех партий\n",
        "    pred = net_out.data.max(1)[1]  # получаем индекс максимального значения\n",
        "    correct += pred.eq(y_var.data).sum()\n",
        "\n",
        "test_loss /= dataset_size\n",
        "print('Average loss: {:.4f}; Accuracy: {}/{} ({:.2f}%)'.format(\n",
        "    test_loss, correct, dataset_size, 100. * correct / dataset_size\n",
        "))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3EV-of-5YaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_error_list = []\n",
        "test_error_list = []\n",
        "nodes_count_list = []\n",
        "\n",
        "nodes_count_from = 200\n",
        "nodes_count_to = 1000\n",
        "delta = 100\n",
        "\n",
        "for nodes_count in np.arange(nodes_count_from, nodes_count_to + 1, delta):\n",
        "  print('Nodes count: {}'.format(nodes_count))\n",
        "  class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.fc1 = nn.Linear(1202, nodes_count)\n",
        "      self.fc2 = nn.Linear(nodes_count, nodes_count)\n",
        "      self.fc3 = nn.Linear(nodes_count, 9)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return F.log_softmax(x, dim=1)\n",
        "\n",
        "  net = Net().double()\n",
        "\n",
        "  # Осуществляем оптимизацию путем стохастического градиентного спуска\n",
        "  # optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "  # Создаем функцию потерь\n",
        "  criterion = nn.NLLLoss()\n",
        "\n",
        "  epochs_count = 10\n",
        "  batch_size = 100\n",
        "\n",
        "  dataset_size = x_test.shape[0]\n",
        "  batches_count = int(np.ceil(dataset_size / batch_size))\n",
        "\n",
        "  for epoch_index in range(epochs_count):\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    for batch_index in range(batches_count):\n",
        "      x_var = Variable(x_train[batch_index * batch_size:batch_index * batch_size + batch_size])\n",
        "      y_var = Variable(y_train[batch_index * batch_size:batch_index * batch_size + batch_size])\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      net_out = net(x_var.double())\n",
        "      loss = criterion(net_out, y_var)\n",
        "\n",
        "      train_loss += loss.item()\n",
        "      pred = net_out.data.max(1)[1]  # получаем индекс максимального значения\n",
        "      correct += pred.eq(y_var.data).sum()\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  train_loss /= dataset_size\n",
        "  train_percentage = 100. * correct.item() / dataset_size\n",
        "  print('Train :: Average loss: {:.4f}; Accuracy: {}/{} ({:.2f}%)'.format(train_loss, correct, dataset_size, train_percentage))\n",
        "\n",
        "\n",
        "  dataset_size = x_test.shape[0]\n",
        "  batch_size = 100\n",
        "  batches_count = int(np.ceil(dataset_size / batch_size))\n",
        "\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  for batch_index in range(batches_count):\n",
        "      x_var = Variable(x_test[batch_index * batch_size:batch_index * batch_size + batch_size])\n",
        "      y_var = Variable(y_test[batch_index * batch_size:batch_index * batch_size + batch_size])\n",
        "\n",
        "      net_out = net(x_var.double())\n",
        "      test_loss += criterion(net_out, y_var).item()  # суммируем потери со всех партий\n",
        "      pred = net_out.data.max(1)[1]  # получаем индекс максимального значения\n",
        "      correct += pred.eq(y_var.data).sum()\n",
        "\n",
        "  test_loss /= dataset_size\n",
        "  test_percentage = 100. * correct.item() / dataset_size\n",
        "  print('Test :: Average loss: {:.4f}; Accuracy: {}/{} ({:.2f}%)'.format(\n",
        "      test_loss, correct, dataset_size, test_percentage\n",
        "  ))\n",
        "\n",
        "  train_error_list.append(100 - train_percentage)\n",
        "  test_error_list.append(100 - test_percentage)\n",
        "  nodes_count_list.append(nodes_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiRpLTaMDtLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (15, 8))\n",
        "plt.plot(nodes_count_list, train_error_list, label = 'Train', color = 'blue')\n",
        "plt.plot(nodes_count_list, test_error_list, label = 'Test', color = 'red')\n",
        "\n",
        "plt.xlabel('Hidden layers size')\n",
        "plt.ylabel('Error')\n",
        "plt.title('Size vs Error')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCISB6D6Mn48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.fc1 = nn.Linear(1202, 400)\n",
        "      self.fc2 = nn.Linear(400, 400)\n",
        "      self.fc3 = nn.Linear(400, 9)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return F.log_softmax(x, dim=1)\n",
        "\n",
        "weight_decays = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8]\n",
        "train_error_list = []\n",
        "test_error_list = []\n",
        "\n",
        "for decay in weight_decays:\n",
        "  print('Weight decay: {}'.format(decay))\n",
        "  net = Net().double()\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=0.001, weight_decay=decay)\n",
        "  criterion = nn.NLLLoss()\n",
        "\n",
        "  epochs_count = 10\n",
        "  batch_size = 100\n",
        "\n",
        "  dataset_size = x_test.shape[0]\n",
        "  batches_count = int(np.ceil(dataset_size / batch_size))\n",
        "\n",
        "  for epoch_index in range(epochs_count):\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    for batch_index in range(batches_count):\n",
        "      x_var = Variable(x_train[batch_index * batch_size:batch_index * batch_size + batch_size])\n",
        "      y_var = Variable(y_train[batch_index * batch_size:batch_index * batch_size + batch_size])\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      net_out = net(x_var.double())\n",
        "      loss = criterion(net_out, y_var)\n",
        "\n",
        "      train_loss += loss.item()\n",
        "      pred = net_out.data.max(1)[1]  # получаем индекс максимального значения\n",
        "      correct += pred.eq(y_var.data).sum()\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  train_loss /= dataset_size\n",
        "  train_percentage = 100. * correct.item() / dataset_size\n",
        "  print('Train :: Average loss: {:.4f}; Accuracy: {}/{} ({:.2f}%)'.format(train_loss, correct, dataset_size, train_percentage))\n",
        "\n",
        "\n",
        "  dataset_size = x_test.shape[0]\n",
        "  batch_size = 100\n",
        "  batches_count = int(np.ceil(dataset_size / batch_size))\n",
        "\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  for batch_index in range(batches_count):\n",
        "      x_var = Variable(x_test[batch_index * batch_size:batch_index * batch_size + batch_size])\n",
        "      y_var = Variable(y_test[batch_index * batch_size:batch_index * batch_size + batch_size])\n",
        "\n",
        "      net_out = net(x_var.double())\n",
        "      test_loss += criterion(net_out, y_var).item()  # суммируем потери со всех партий\n",
        "      pred = net_out.data.max(1)[1]  # получаем индекс максимального значения\n",
        "      correct += pred.eq(y_var.data).sum()\n",
        "\n",
        "  test_loss /= dataset_size\n",
        "  test_percentage = 100. * correct.item() / dataset_size\n",
        "  print('Test :: Average loss: {:.4f}; Accuracy: {}/{} ({:.2f}%)'.format(\n",
        "      test_loss, correct, dataset_size, test_percentage\n",
        "  ))\n",
        "\n",
        "  train_error_list.append(100 - train_percentage)\n",
        "  test_error_list.append(100 - test_percentage)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVhhzHweS3sn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_error_list = train_error_list[-8:]\n",
        "test_error_list = test_error_list[-8:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ritpYzjKOvMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = (15, 8))\n",
        "plt.plot(weight_decays, train_error_list, label = 'Train', color = 'blue')\n",
        "plt.plot(weight_decays, test_error_list, label = 'Test', color = 'red')\n",
        "\n",
        "plt.xlabel('Weight decay')\n",
        "plt.ylabel('Error')\n",
        "plt.title('Weight decay vs Error')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}